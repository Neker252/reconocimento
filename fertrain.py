import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.losses import categorical_crossentropy
from keras.optimizers import RMSprop
from keras.regularizers import l2
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import EarlyStopping

# Configuraci√≥n
num_features = 64
num_labels = 7
batch_size = 64
epochs = 30   # M√°s √©pocas porque early stopping controlar√°
width, height = 48, 48

train_dir = 'train'
test_dir = 'test'

emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

X = []
Y = []

print("‚è≥ Cargando im√°genes...")

for idx, emotion in enumerate(emotions):
    folder_path = os.path.join(train_dir, emotion)
    
    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è No existe la carpeta: {folder_path}")
        continue

    for img_name in os.listdir(folder_path):
        img_path = os.path.join(folder_path, img_name)
        try:
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            img = cv2.resize(img, (48, 48))
            X.append(img)
            Y.append(idx)
        except:
            continue

# Convertir a numpy
x = np.array(X, dtype='float32') / 255.0
y = np_utils.to_categorical(Y, num_classes=num_labels)

# ========== REDUCIR DATOS PARA PRUEBA R√ÅPIDA ==========
print(f"\nüìä Im√°genes originales cargadas: {len(x)}")

# Toma solo el 20% para pruebas r√°pidas
subset_size = len(x) // 5  # 20%
x = x[:subset_size]
y = y[:subset_size]

print(f"üöÄ Usando {len(x)} im√°genes para prueba r√°pida (20% del total)")
print(f"‚úÖ Esto reducir√° el tiempo de entrenamiento aprox. 80%")
# ========== FIN REDUCCI√ìN ==========

# Normalizaci√≥n
x -= np.mean(x)
x /= (np.std(x) + 1e-7)

# Formato para CNN (N, 48, 48, 1)
x = x.reshape(-1, width, height, 1)

print("‚úÖ Datos cargados:")
print("x:", x.shape)
print("y:", y.shape)

# Dividir en sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=41)

print(f"üìä Divisiones:")
print(f"  Entrenamiento: {len(X_train)} im√°genes")
print(f"  Validaci√≥n: {len(X_valid)} im√°genes")
print(f"  Prueba: {len(X_test)} im√°genes")

# Guardar muestras de test
np.save('modXtest.npy', X_test)
np.save('modytest.npy', y_test)

# ========== MODELO ==========
model = Sequential()

model.add(Conv2D(num_features, (3, 3), activation='relu', input_shape=(width, height, 1), kernel_regularizer=l2(0.01)))
model.add(Conv2D(num_features, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(2*num_features, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(2*num_features, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(4*num_features, (3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(4*num_features, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='tanh'))
model.add(Dense(64, activation='relu'))
model.add(Dense(num_labels, activation='softmax'))

# ========== COMPILAR CON RMSprop ==========
model.compile(
    loss=categorical_crossentropy,
    optimizer=RMSprop(lr=0.0005),  # Optimizador para im√°genes
    metrics=['accuracy']
)

print("\n" + "="*60)
print("CONFIGURACI√ìN DEL MODELO:")
print("="*60)
print(f"‚Ä¢ Optimizador: RMSprop (lr=0.0005)")
print(f"‚Ä¢ √âpocas m√°ximas: {epochs}")
print(f"‚Ä¢ Batch size: {batch_size}")
print(f"‚Ä¢ Neuronas Dense: 256 ‚Üí 128 ‚Üí 64")
print(f"‚Ä¢ Activaciones: relu ‚Üí tanh ‚Üí relu")
print("="*60 + "\n")

print("‚úÖ Configurando Early Stopping (para en 5 √©pocas sin mejorar)")
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=9,
    restore_best_weights=True,
    verbose=1
)

print("üöÄ Entrenando modelo con AUMENTO DE DATOS y EARLY STOPPING...")

# ========== AUMENTO DE DATOS ==========
datagen = ImageDataGenerator(
    rotation_range=15,  
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

# Calcular steps por √©poca
steps_per_epoch = len(X_train) // batch_size
print(f"üìä Steps por √©poca: {steps_per_epoch}")

# Entrenar con fit_generator (m√°s compatible)
history = model.fit_generator(
    generator=datagen.flow(X_train, y_train, batch_size=batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    verbose=1,
    validation_data=(X_valid, y_valid),
    callbacks=[early_stopping]
)

# Guardar modelo
fer_json = model.to_json()
with open("fer.json", "w") as json_file:
    json_file.write(fer_json)

model.save_weights("fer.h5")

print("\n" + "="*60)
print("RESUMEN DEL ENTRENAMIENTO:")
print("="*60)
print(f"‚úÖ Modelo guardado: fer.json y fer.h5")
print(f"‚Ä¢ √âpocas ejecutadas: {len(history.history['loss'])} de {epochs}")
if 'val_acc' in history.history:
    print(f"‚Ä¢ Mejor precisi√≥n en validaci√≥n: {max(history.history['val_acc'])*100:.2f}%")
if 'val_loss' in history.history:
    print(f"‚Ä¢ P√©rdida final en validaci√≥n: {history.history['val_loss'][-1]:.4f}")
print("="*60)

# Probar inmediatamente
print("\nüéØ Probando modelo con datos de test...")
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"‚úÖ Precisi√≥n en test: {test_acc*100:.2f}%")
print(f"‚úÖ P√©rdida en test: {test_loss:.4f}")